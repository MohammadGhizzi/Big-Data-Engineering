{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97fa671b-66c0-48b3-9d1b-435e1467b8b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"ML Model\")\n",
    "         .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f8d044a-b3d4-4c29-9e4b-b497cc64828b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "posts = spark.read.parquet(\"/mnt/SultanAlmalki/Posts/*\")\n",
    "ml_model = \"/mnt/SultanAlmalki/model\"\n",
    "stringindexer = \"/mnt/SultanAlmalki/stringindexer\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfd5773a-c817-411d-882c-e4b7bbbbce24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# User defined function\n",
    "def predictions_udf(df, ml_model, stringindexer):\n",
    "    from pyspark.sql.functions import col, regexp_replace, lower, trim\n",
    "    from pyspark.ml import PipelineModel\n",
    "\n",
    "    # Filter out empty body text\n",
    "    df = df.filter(\"Body is not null\")\n",
    "    # Making sure the naming of the columns are consistent with the model\n",
    "    df = df.select(col(\"Body\").alias(\"text\"), col(\"Tags\"))\n",
    "    # Preprocessing of the feature column\n",
    "    cleaned = df.withColumn('text', regexp_replace('text', r\"http\\S+\", \"\")) \\\n",
    "                    .withColumn('text', regexp_replace('text', r\"[^a-zA-z]\", \" \")) \\\n",
    "                    .withColumn('text', regexp_replace('text', r\"\\s+\", \" \")) \\\n",
    "                    .withColumn('text', lower('text')) \\\n",
    "                    .withColumn('text', trim('text')) \n",
    "\n",
    "    # Load in the saved pipeline model\n",
    "    model = PipelineModel.load(ml_model)\n",
    "\n",
    "    # Making the prediction\n",
    "    prediction = model.transform(df)\n",
    "\n",
    "    predicted = prediction.select(col('text'), col('Tags'), col('prediction'))\n",
    "\n",
    "    # Decoding the indexer\n",
    "    from pyspark.ml.feature import StringIndexerModel, IndexToString\n",
    "\n",
    "    # Load in the StringIndexer that was saved\n",
    "    indexer = StringIndexerModel.load(stringindexer)\n",
    "\n",
    "    # Initialize the IndexToString converter\n",
    "    i2s = IndexToString(inputCol = 'prediction', outputCol = 'decoded', labels = indexer.labels)\n",
    "    converted = i2s.transform(predicted)\n",
    "\n",
    "    # Display the important columns\n",
    "    return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d57a53e-7761-40ac-9bc3-12c2b1b76434",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = predictions_udf(posts,ml_model, stringindexer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcb9080f-1657-48ba-9197-eff4a5836123",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# change the column name \n",
    "topics = result.withColumnRenamed('decoded', 'topic').select('topic')\n",
    "\n",
    "# Aggregate the topics and calculate the total qty of each topic\n",
    "topic_qty = topics.groupBy(col(\"topic\")).agg(count('topic').alias('qty')).orderBy(desc('qty'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d19ff5cd-5365-468c-84ec-c50c5822ce84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# define this function\n",
    "\n",
    "def crt_sgl_file(result_path):\n",
    "        # write the result to a folder container several files\n",
    "        path = \"/mnt/SultanAlamlki/BI/ml_result\"\n",
    "        topic_qty.write.option(\"delimiter\", \",\").option(\"header\", \"true\").mode(\"overwrite\").csv(path)\n",
    "\n",
    "        # list the folder, find the csv file \n",
    "        filenames = dbutils.fs.ls(path)\n",
    "        name = ''\n",
    "        for filename in filenames:\n",
    "            if filename.name.endswith('csv'):\n",
    "                org_name = filename.name\n",
    "\n",
    "        # copy the csv file to the path you want to save, in this example, we use  \"/mnt/deBDProject/BI/ml_result.csv\"\n",
    "        dbutils.fs.cp(path + '/'+ org_name, result_path)\n",
    "\n",
    "        # delete the folder\n",
    "        dbutils.fs.rm(path, True)\n",
    "\n",
    "        print('single file created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83eb7649-d18c-4a37-8677-564e62600dd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single file created\n"
     ]
    }
   ],
   "source": [
    "# run the function\n",
    "result_path = \"/mnt/SultanAlmalki/BI/ml_result.csv\"\n",
    "\n",
    "crt_sgl_file(result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e31d6fe-b3e5-44b9-8fdd-dbd458016add",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = predictions_udf(posts, ml_model, stringindexer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b6899f5-1d07-45a6-9d56-709904998ab6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Category:\n",
    "categorize the data and count the quantity in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c818007a-7e3d-4ee5-9ae9-eb73ccf8e913",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, sum as sum_func\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Topic Categorization\").getOrCreate()\n",
    "\n",
    "ml_result_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"/mnt/SultanAlmalki/BI/ml_result.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e28d8e44-ece8-4bea-8a15-b8c097999315",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def categorize_topic(topic):\n",
    "    return when(topic.isin(\"c#\", \"java\", \"javascript\", \"c++\", \"python\", \"objective-c\", \"ruby\", \"c\"), \"Programming Languages\") \\\n",
    "           .when(topic.isin(\"hibernate\", \"jquery\", \"linq\"), \"Frameworks and Libraries\") \\\n",
    "           .when(topic.isin(\"php\", \"asp.net\", \"ruby-on-rails\", \".net\", \"kohana\", \"drupal\", \"cakephp\", \"silverlight\"), \"Web Development\") \\\n",
    "           .when(topic.isin(\"android\", \"iphone\", \"ios\", \"cocoa-touch\", \"blackberry\", \"windows-phone-7\"), \"Mobile Development\") \\\n",
    "           .when(topic.isin(\"mysql\", \"sql\", \"ms-access\", \"sql-server-2008\", \"sql-server\", \"sqlite\", \"database\"), \"Databases\") \\\n",
    "           .when(topic.isin(\"wpf\", \"winforms\", \"winapi\", \"forms\"), \"Desktop Development\") \\\n",
    "           .when(topic == \"hadoop\", \"Big Data\") \\\n",
    "           .when(topic.isin(\"heap\", \"algorithm\"), \"Data Structures and Algorithms\") \\\n",
    "           .when(topic == \"pdf\", \"File Formats\") \\\n",
    "           .when(topic == \"matlab\", \"Scientific Computing\") \\\n",
    "           .when(topic == \"gps\", \"Location and Mapping\") \\\n",
    "           .when(topic == \"mvvm\", \"Design Patterns\") \\\n",
    "           .when(topic.isin(\"visual-studio\", \"eclipse\", \"xcode\"), \"IDEs and Tools\") \\\n",
    "           .when(topic.isin(\"categories\", \"properties\", \"regex\", \"performance\", \"warnings\"), \"General\") \\\n",
    "           .when(topic.isin(\"reportingservices-2005\", \"reporting-services\"), \"Reporting\") \\\n",
    "           .when(topic == \"plot\", \"Data Visualization\") \\\n",
    "           .when(topic == \"audio\", \"Multimedia\")\\\n",
    "           .otherwise(\"Other\")\n",
    "\n",
    "categorized_df = ml_result_df.withColumn(\"category\", categorize_topic(col(\"topic\")))\n",
    "category_totals = categorized_df.groupBy(\"category\").agg(sum_func(\"qty\").alias(\"total_qty\")).orderBy(\"total_qty\", ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f039c86-fc1e-4d14-8afc-bdaf53a89203",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Repartition the DataFrame into a single partition\n",
    "category_totals = category_totals.coalesce(1)\n",
    "\n",
    "# Save the result as a CSV file in your Azure Storage\n",
    "output_path = \"/mnt/SultanAlmalki/BI/categorized_results.csv\"\n",
    "category_totals.write.option(\"header\", \"true\").mode(\"overwrite\").csv(output_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Model Deployment",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
